{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electrical-method",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "- It is a process where we automatically select those features in our data that contribute most to the prediction variable or output in which we are interested.\n",
    "\n",
    "## Benefits of feature selection\n",
    "* **Reduces Overfitting** : Less redundant data means less opportunity to make decisions based on noise.\n",
    "* **Improves Accuracy** : Less misleading data means modelling accuracy improves.\n",
    "* **Reduces Training Time** : Less data means that algorithms train faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-daily",
   "metadata": {},
   "source": [
    "## Automatic feature selection techniquies are : \n",
    "* Univariate Selection\n",
    "* Recursive Feature Elimination\n",
    "* Principle Component Analysis\n",
    "* Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-diabetes",
   "metadata": {},
   "source": [
    "### 1 . Univariate Selection\n",
    "* Statistical tests can be used to select those features that have the strongest relationship with the output variable.\n",
    "* Scikit-learn library provides the **SelectKBest** class that can be used with a suite of different statistical tests to select a specific number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "received-tunnel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with Univariate Statistical tests(chi-squared for classification)\n",
    "\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dataframe = read_csv(filename,names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# feature extraction\n",
    "# here k=4 specifies that we are trying the extract the best 4 features\n",
    "test = SelectKBest(score_func=chi2,k=4)\n",
    "fit = test.fit(X,Y)\n",
    "\n",
    "# summarizing the scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "\n",
    "# Top four scores are our best features\n",
    "\n",
    "features = fit.transform(X)\n",
    "\n",
    "# summarizing selected features \n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-serial",
   "metadata": {},
   "source": [
    "### 3 . Recursive Feature Elimination\n",
    "* It works by recursively removing attributes and buiding a model on those attributes that remain.\n",
    "* It uses the model accuracy to identify which attributes contribute the most to predicting the target attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exempt-kingston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:72: FutureWarning: Pass n_features_to_select=3 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# feature extraction with RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model,3)\n",
    "fit = rfe.fit(X,Y)\n",
    "\n",
    "print(\"Num Features: %d\" %(fit.n_features_)) \n",
    "print(\"Selected Features: %s\" %(fit.support_)) \n",
    "print(\"Feature Ranking: %s\"  %(fit.ranking_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-entrance",
   "metadata": {},
   "source": [
    "### 3 . Principal Component Analysis\n",
    "        (data reduction technique)\n",
    "* It uses linear algebra to transform the dataset into a compressed form.\n",
    "* A property of PCA is that we can choose the number of dimensions or principal components in transformed result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "generous-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance : [0.889 0.062 0.026]\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# summarizing components\n",
    "print(\"Explained Variance : %s\"%fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-browse",
   "metadata": {},
   "source": [
    "### 4 . Feature Importance\n",
    "* Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wired-austin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11  0.239 0.098 0.08  0.074 0.135 0.121 0.144]\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,Y)\n",
    "# larger the score the more important the attribute\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-charles",
   "metadata": {},
   "source": [
    "# Summary\n",
    "* learnt about 4 different automatic feature selection techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-wealth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
