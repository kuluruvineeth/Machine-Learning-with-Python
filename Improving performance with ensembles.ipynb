{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regulation-estimate",
   "metadata": {},
   "source": [
    "# Improving performance with ensembles\n",
    "* Ensembles can give us the boost in accuracy on our dataset.\n",
    "## Combining Models Into Ensemble Predictions\n",
    "* The three most popular methods for combining the predictions from different models are  :\n",
    "### Bagging\n",
    "* Building Multiple models from different subsamples of the training dataset.\n",
    "### Boosting\n",
    "* Building multiple models each of which learns to fix the prediction errors of a prior model in the sequence of models.\n",
    "### Voting\n",
    "* Building multiple models and simple statistics(like calculating the mean) are used to combine predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-yeast",
   "metadata": {},
   "source": [
    "## 1. Bagging Algorithms\n",
    "* **Bootstrap Aggregation(Bagging)** involves taking multiple samples from our training dataset and training a model for each sample.\n",
    "* **The final output prediction is averaged across the predictions of all sub-models**.\n",
    "* The three bagging models are : \n",
    "    * **Bagged Decision Trees**\n",
    "    * **Random Forest**\n",
    "    * **Extra Trees**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-johns",
   "metadata": {},
   "source": [
    "### 1.1 Bagged Decision Trees\n",
    "* Bagging performs best with algorithms that have high variance.\n",
    "* A popular example are decision trees,often constructed without pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-wales",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7577238550922762\n"
     ]
    }
   ],
   "source": [
    "## Bagged Decision Trees for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dataframe = read_csv(filename,names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10,random_state=None)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart,n_estimators=num_trees,random_state=None)\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-spirituality",
   "metadata": {},
   "source": [
    "### 1.2 Random Forest\n",
    "* It is an extension of bagged decision trees\n",
    "* Samples of the training dataset are taken with replacement,but the trees are constructed in a way that reduces the correlation between individual classifiers.\n",
    "* Only a random subset of features are considered for each split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "educational-county",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7499829118250172\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## split points chosen from a random selection of 3 features.\n",
    "max_features = 3\n",
    "model = RandomForestClassifier(n_estimators=num_trees,max_features=max_features)\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-vitamin",
   "metadata": {},
   "source": [
    "### 1.3 Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "presidential-cleaners",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7512645249487355\n"
     ]
    }
   ],
   "source": [
    "# Extra Trees Classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "max_features = 7\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees,max_features=max_features)\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-jacob",
   "metadata": {},
   "source": [
    "## 2. Boosting Algorithms\n",
    "* It creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence.\n",
    "* The two most common boosting ensemble machine learning algorithms are:\n",
    "    * **AdaBoost**\n",
    "    * **Stochastic Gradient Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-quarter",
   "metadata": {},
   "source": [
    "### 2.1 AdaBoost\n",
    "* It generally works by weighting instances in the dataset by how easy or difficult they are to classify,allowing the algorithm to pay or less attention to them in the construction of subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "standard-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760457963089542\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "num_trees = 30\n",
    "model = AdaBoostClassifier(n_estimators=num_trees,random_state=None)\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-aurora",
   "metadata": {},
   "source": [
    "### 2.2 Stochastic Gradient Boosting\n",
    "* Also called Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accredited-anderson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7642857142857143\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "num_trees = 100\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees,random_state=None)\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-catch",
   "metadata": {},
   "source": [
    "## 3. Voting Ensemble\n",
    "* Voting is one of the simplest way of combining the predictions from multiple machine learning algorithms.\n",
    "* It works by first creating two or more standalone models from our training dataset.\n",
    "* A voting classifier can then be used to wrap our models and average the predictions of the sub-models when asked to make predictions for new data.\n",
    "* More advanced methods can learn how to best weight the predictions from the sub-models,this is called stacking(stacked aggregation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quiet-monitoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669002050580997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic',model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart',model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm',model3))\n",
    "\n",
    "# create the estimate model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-needle",
   "metadata": {},
   "source": [
    "# Summary\n",
    "* We learned about :\n",
    "    * Bagging ensembles including Bagged Decision Trees,Random Forest and Extra Trees\n",
    "    * Boo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
