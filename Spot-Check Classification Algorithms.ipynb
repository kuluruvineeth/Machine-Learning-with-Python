{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "laughing-fetish",
   "metadata": {},
   "source": [
    "# Spot-Checking Classification Algorithms\n",
    "* It is a way of discovering which algorithms perform well on our machine learning problem.\n",
    "* We must trial a number of methods and focus attention on those that prove themselves the most promising.\n",
    "* We must use trial and error to discover a shortlist of algorithms that do well on our problem that we can can double down on and tune further.This process is called **spot-checking**.\n",
    "* The question we are going to answer is : \n",
    "    * **what algorithms should i spot-check on my dataset?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-seating",
   "metadata": {},
   "source": [
    "## Algorithms Overview\n",
    "we are going to take a look at six classification algorithms that we can spot-check on our dataset.\n",
    "\n",
    "**1. Two linear ML algoithms**\n",
    "  * Logistic Regression\n",
    "  * Linear Discriminant Analysis\n",
    "  \n",
    "**2. Four Nonlinear ML algorithms**\n",
    "  * K-nearest Neighbors\n",
    "  * Naive Bayes\n",
    "  * Classification and Regression Trees\n",
    "  * Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-hindu",
   "metadata": {},
   "source": [
    "### 1.1 Logistic Regression\n",
    "* Logistic Regression assumes a Gaussian distribution for the numeric input variables and can model binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "obvious-termination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773427887901572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classification\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dataframe = read_csv(filename,names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10,random_state=None)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-nickel",
   "metadata": {},
   "source": [
    "### 1.2 Linear Discriminant Analysis\n",
    "* It is a statistical technique for binary and multiclass classification.\n",
    "* It too assumes Gaussian distribution for the numerical input variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fundamental-economy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064251538\n"
     ]
    }
   ],
   "source": [
    "# LDA Classification\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model =  LinearDiscriminantAnalysis()\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-appreciation",
   "metadata": {},
   "source": [
    "### 2.1 K-Nearest Neighbours\n",
    "* It uses a distance metric to find the k most similar instances in the training data for a new instance and takes the mean outcome of the neighbors as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "liable-filter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7265550239234451\n"
     ]
    }
   ],
   "source": [
    "## KNN classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-mitchell",
   "metadata": {},
   "source": [
    "### 2.2 Naive Bayes\n",
    "* It calculates the **probability of each class** and the **conditional probability of each class given each input value**.\n",
    "* These probabilities are estimated for new data and multiplied together,assuming that they are all **independent(a simple or naive assumption)**.\n",
    "* When working with real-valued data,a Gaussian distribution is assumed to easily estimate the **probabilities for input variables using Gaussian Probability Density Function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "contrary-lindsay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7551777170198223\n"
     ]
    }
   ],
   "source": [
    "## Gaussian Naive Bayes Classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-exhaust",
   "metadata": {},
   "source": [
    "### 2.3 Classification and Regression Trees\n",
    "* CART construct a binary tree from the training data.\n",
    "* Split points are choosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (**like Gini index**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "provincial-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6978298017771701\n"
     ]
    }
   ],
   "source": [
    "## CART Classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-parker",
   "metadata": {},
   "source": [
    "### 2.4 Support Vector Machines\n",
    "* It seek a line that best separates two classes.\n",
    "* Those data instances that are close to the line that best separates the classes are called **support vectors** and influence where the line is placed.\n",
    "* Of paricular importance is the use of **different kernal functions** via the **kernel parameter**.\n",
    "* A **powerful Radial Basis Function** is used by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "velvet-flush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7604237867395763\n"
     ]
    }
   ],
   "source": [
    "## SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-platinum",
   "metadata": {},
   "source": [
    "# Summary\n",
    "* learnt how to spot-check on 6 machine learning classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-guard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
